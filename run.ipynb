{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "seed = 2453466\n",
    "checkpt_dir = 'checkpoints'\n",
    "dataset = 'MNIST'\n",
    "num_experts = 4\n",
    "input_size = 28 * 28\n",
    "\n",
    "# Initialization hyper-parameters\n",
    "load_initialized_experts = False\n",
    "init_expert_model = 'blockmodel'\n",
    "init_optimizer = 'sgd'\n",
    "init_learning_rate = .01\n",
    "init_weight_decay = 0\n",
    "init_epochs = 1\n",
    "\n",
    "# Training hyper-parameters\n",
    "discriminator_optimizer = 'sgd'\n",
    "discriminator_learning_rate = .01\n",
    "discriminator_weight_decay = 0\n",
    "\n",
    "expert_optimizer = 'sgd'\n",
    "expert_learning_rate = .01\n",
    "expert_weight_decay = 0\n",
    "\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import MNISTDataset\n",
    "\n",
    "train_dataset = MNISTDataset(train=True, transformer_names=[\"rotate_left\", \"rotate_left\"])\n",
    "test_dataset = MNISTDataset(train=False, transformer_names=[\"rotate_left\", \"rotate_left\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init seed and training device\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "torch.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for checkpoints\n",
    "if not os.path.exists(checkpt_dir):\n",
    "    os.mkdir(checkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize experts\n",
    "from model import Expert, Discriminator\n",
    "experts = [Expert(dataset=dataset, input_size=input_size, optim=init_optimizer, lr=init_learning_rate, weight_decay=init_weight_decay).to(device) for _ in range(num_experts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "loss_initial = torch.nn.MSELoss(reduction='mean')\n",
    "criterion = torch.nn.BCELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir = 'results'\n",
    "# name = 'test'\n",
    "# log_dir = os.path.join(outdir, 'logs')\n",
    "# if not os.path.exists(log_dir):\n",
    "#     os.mkdir(log_dir)\n",
    "# log_dir_exp = os.path.join(log_dir, name)\n",
    "# os.mkdir(log_dir_exp)\n",
    "# writer = SummaryWriter(log_dir=log_dir_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     writer.add_scalar(f\"num\", i+4, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Experts as approximately Identity on Transformed Data\n",
    "from trainer import initialize_expert\n",
    "\n",
    "\n",
    "for i, expert in enumerate(experts):\n",
    "    pass\n",
    "    # if load_initialized_experts:\n",
    "    #     path = os.path.join(checkpt_dir, f'{model_for_initialized_experts}_E_{i+1}_init.pth')\n",
    "    #     init_weights(expert, path)\n",
    "    # else:\n",
    "    #     initialize_expert(\n",
    "    #         epochs=init_epochs, \n",
    "    #         architecture_name=init_expert_model, \n",
    "    #         expert=expert, \n",
    "    #         i=i, \n",
    "    #         loss=loss_initial, \n",
    "    #         data_train=train_loader,\n",
    "    #         device=device,\n",
    "    #         checkpt_dir=checkpt_dir,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Discriminator\n",
    "discriminator = Discriminator(dataset=dataset, input_size=input_size, \n",
    "                                optim=discriminator_optimizer, \n",
    "                                lr=discriminator_learning_rate, \n",
    "                                weight_decay=discriminator_weight_decay).to(device)\n",
    "\n",
    "# Optimizers\n",
    "for expert in experts:\n",
    "    expert.set_optimizer(optim=expert_optimizer, lr=expert_learning_rate, weight_decay=expert_weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1] loss_D_transformed 0.6813\n",
      "epoch [1] loss_D_canon 0.7164\n",
      "epoch [1] loss_D_transformed 0.6829\n",
      "epoch [1] loss_D_canon 0.7020\n",
      "epoch [1] loss_D_transformed 0.6850\n",
      "epoch [1] loss_D_canon 0.6894\n",
      "epoch [1] loss_D_transformed 0.6866\n",
      "epoch [1] loss_D_canon 0.6769\n",
      "epoch [1] loss_D_transformed 0.6884\n",
      "epoch [1] loss_D_canon 0.6657\n",
      "epoch [1] loss_D_transformed 0.6901\n",
      "epoch [1] loss_D_canon 0.6545\n",
      "epoch [1] loss_D_transformed 0.6918\n",
      "epoch [1] loss_D_canon 0.6438\n",
      "epoch [1] loss_D_transformed 0.6935\n",
      "epoch [1] loss_D_canon 0.6327\n",
      "epoch [1] loss_D_transformed 0.6953\n",
      "epoch [1] loss_D_canon 0.6222\n",
      "epoch [1] loss_D_transformed 0.6969\n",
      "epoch [1] loss_D_canon 0.6128\n",
      "epoch [1] loss_D_transformed 0.6983\n",
      "epoch [1] loss_D_canon 0.6032\n",
      "epoch [1] loss_D_transformed 0.6999\n",
      "epoch [1] loss_D_canon 0.5931\n",
      "epoch [1] loss_D_transformed 0.7017\n",
      "epoch [1] loss_D_canon 0.5835\n",
      "epoch [1] loss_D_transformed 0.7033\n",
      "epoch [1] loss_D_canon 0.5739\n",
      "epoch [1] loss_D_transformed 0.7050\n",
      "epoch [1] loss_D_canon 0.5637\n",
      "epoch [1] loss_D_transformed 0.7066\n",
      "epoch [1] loss_D_canon 0.5545\n",
      "epoch [1] loss_D_transformed 0.7081\n",
      "epoch [1] loss_D_canon 0.5458\n",
      "epoch [1] loss_D_transformed 0.7097\n",
      "epoch [1] loss_D_canon 0.5368\n",
      "epoch [1] loss_D_transformed 0.7112\n",
      "epoch [1] loss_D_canon 0.5282\n",
      "epoch [1] loss_D_transformed 0.7128\n",
      "epoch [1] loss_D_canon 0.5193\n",
      "epoch [1] loss_D_transformed 0.7144\n",
      "epoch [1] loss_D_canon 0.5108\n",
      "epoch [1] loss_D_transformed 0.7164\n",
      "epoch [1] loss_D_canon 0.5014\n",
      "epoch [1] loss_D_transformed 0.7179\n",
      "epoch [1] loss_D_canon 0.4934\n",
      "epoch [1] loss_D_transformed 0.7194\n",
      "epoch [1] loss_D_canon 0.4858\n",
      "epoch [1] loss_D_transformed 0.7211\n",
      "epoch [1] loss_D_canon 0.4778\n",
      "epoch [1] loss_D_transformed 0.7227\n",
      "epoch [1] loss_D_canon 0.4705\n",
      "epoch [1] loss_D_transformed 0.7243\n",
      "epoch [1] loss_D_canon 0.4633\n",
      "epoch [1] loss_D_transformed 0.7260\n",
      "epoch [1] loss_D_canon 0.4562\n",
      "epoch [1] loss_D_transformed 0.7281\n",
      "epoch [1] loss_D_canon 0.4486\n",
      "epoch [1] loss_D_transformed 0.7297\n",
      "epoch [1] loss_D_canon 0.4421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7001b74cb951>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_system\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# if epoch % args.log_interval == 0 or epoch == args.epochs-1:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# torch.save(discriminator.state_dict(), checkpt_dir + '/{}_D.pth'.format(args.name))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\University\\B.Sc. project\\Code\\recurrent-experts\\trainer.py\u001b[0m in \u001b[0;36mtrain_system\u001b[1;34m(epoch, experts, discriminator, criterion, data_train, input_size, device)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mloss_D_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_D_transformed\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_experts\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_experts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mtotal_loss_D_transformed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_D_transformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mloss_D_transformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trainer import *\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    train_system(epoch, experts, discriminator, criterion, train_loader, input_size, device)\n",
    "    # if epoch % args.log_interval == 0 or epoch == args.epochs-1:\n",
    "        # torch.save(discriminator.state_dict(), checkpt_dir + '/{}_D.pth'.format(args.name))\n",
    "        # for i in range(args.num_experts):\n",
    "            # torch.save(experts[i].state_dict(), checkpt_dir + '/{}_E_{}.pth'.format(args.name, i+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
